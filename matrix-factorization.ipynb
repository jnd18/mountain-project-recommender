{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendations via Matrix Factorization\n",
    "\n",
    "## Introduction\n",
    "This notebook explains the main ideas behind a common class of algorithms for building recommender systems.\n",
    "Assume we have a data set of triples (user, item, rating). \n",
    "The main idea is to represent this data as a matrix $X$, where the entry in the $i$th row and $j$th column, $X_{ij}$,\n",
    "is the rating that user $i$ gave to item $j$.\n",
    "It is almost never the case that every user has rated every item, so $X$ has many missing values.\n",
    "The goal is to learn how to fill in the missing values.\n",
    "In other words, we want to figure out which items a user hasn't rated but would rate highly, so that we can recommend these items to them.\n",
    "\n",
    "We do this by finding matrices $P$ and $Q$ such that $X \\approx PQ$.\n",
    "Why would this be useful? \n",
    "The trick is that both $P$ and $Q$ are supposed to be \"low rank.\"\n",
    "That means, if $X$ is an $m \\times n$ matrix, then $P$ is $m \\times k$ and $Q$ is $k \\times n$ \n",
    "for some $k$ typically much smaller than either $m$ or $n$.\n",
    "Thus, we are trying to capture the patterns present in (up to) $mn$ numbers, using only $k(m + n)$ parameters.\n",
    "Provided $k$ is small enough, there is clearly some kind of compression or distillation of information,\n",
    "so it makes sense (to me, anyway) to claim that we've learned something about $X$.\n",
    "We can then fill in the missing values of $X$ with the corresponding values of $PQ$.\n",
    "As for exactly what we've learned, I'll talk about below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "The matrix factorization procedure outlined above actually has a nifty geometric interpretation.\n",
    "Suppose, as we mentioned above, that $X$ is an $m \\times n$ matrix.\n",
    "That means there are $m$ users and $n$ items.\n",
    "Since $P$ is $m \\times k$, $P$ has a row for every user.\n",
    "Similarly, since $Q$ is $k \\times n$, $Q$ has a column for every item.\n",
    "So, every user and every item gets assigned a $k$-dimensional vector, corresponding to a row or column of $P$ or $Q$ respectively.\n",
    "Thus, when we factorize $X$, we're learning vector representations of our users and items in a $k$-dimensional latent space.\n",
    "\n",
    "Now, letting $p_i$ denote row $i$ of $P$, and letting $q_j$ denote column $j$ of $Q$,\n",
    "$X_{ij} \\approx [PQ]_{ij} = p_i \\cdot q_j$.\n",
    "This means that our estimated rating for user $i$ on item $j$ is simply the dot product of their two vector representations.\n",
    "Now, you may remember that the dot product between two vectors is the product of their lengths times the cosine of the angle between them.\n",
    "Thus, ignoring vector length for a moment, if the vectors for a given user and item point in generally the same direction,\n",
    "then we predict that the user will rate that item highly.\n",
    "Items that are usually rated highly will have longer vector representations.\n",
    "Similarly, users that tend to rate items more highly will have longer vector representations.\n",
    "\n",
    "So factoring the matrix learns a representation with natural geometric properties.\n",
    "Neat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Details\n",
    "\n",
    "Ok, now we're ready for the nitty gritty. \n",
    "\n",
    "The formal statement of the problem is to find $P$ and $Q$ minimizing \n",
    "$$\n",
    "L(X, P, Q, \\lambda) := \\sum_{i,j} (X_{ij} - [PQ]_{ij}) ^ 2 + \\lambda (||P||^2 + ||Q||^2),\n",
    "$$\n",
    "where the sum is over only the observed entries of $X$, and $\\lambda$ is a hyperparameter controlling the amount of $L_2$ regularization.\n",
    "\n",
    "We can optimize this loss function via gradient descent, deep learning style. So, let's figure out the gradient.\n",
    "(I skipped some steps. You can either take my word for it, or grab a piece of paper. It's not too bad.)\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial P_{il}} \n",
    "& = \\sum_j \\frac{\\partial}{\\partial P_{il}} (X_{ij} - [PQ]_{ij}) ^ 2 + \\frac{\\partial}{\\partial P_{il}} \\lambda ||P||^2 \\\\\n",
    "& = -\\sum_j 2 (X_{ij} - [PQ]_{ij}) Q_{lj} + 2 \\lambda P_{il}\n",
    "\\end{align}\n",
    "\n",
    "where the sum is over all the $j$ where the $ij$ entry of $X$ is known.\n",
    "\n",
    "Similarly,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Q_{lj}} = -\\sum_i 2 (X_{ij} - [PQ]_{ij}) P_{il} + 2 \\lambda Q_{lj},\n",
    "$$\n",
    "\n",
    "where the sum is over all the $i$ where the $ij$ entry of $X$ is known.\n",
    "\n",
    "And that's really all we need to perform gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Implementation\n",
    "\n",
    "So, now that we've discussed the mathematical details, we have all the information we need to actually implement a matrix factorization algorithm.\n",
    "I do this in NumPy, below. My implementation is not terribly efficient or rigorously tested. \n",
    "But I'm not trying to show best practices here. I'm just trying to show a sketch the theoretical ideas in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $X$ will generally be very sparse, and we need a way to keep track of which entries are missing, I'll simply represent $X$ as a dictionary\n",
    "where the keys are $(i, j)$ pairs, and the values are the $X_{ij}$ entries. The absence of a key denotes that the corresponding entry is not known.\n",
    "\n",
    "Since $P$ and $Q$ will be non-sparse, we can just use ordinary numpy arrays for those, but we'll transpose $Q$, so that way\n",
    "there is symmetry in our handling of the two. \n",
    "In particular, since NumPy lets us index rows like `P[i]`, making the rows the import vectors for both $P$ and $Q$ makes the indexing a little nicer to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the loss function. That first line is kind of a doozy. I'm using the `zip(*x)` trick to unzip or transpose the list of tuples. This turns the list of 3-tuples into 3 very long tuples. But then I have to convert those tuples into lists for the indexing to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, P, Q, l2):\n",
    "    i, j, x = (list(a) for a in zip(*[(i, j, x) for (i, j), x in X.items()]))\n",
    "    Xhat = np.sum(P[i] * Q[j], axis=1) # row-wise dot product\n",
    "    return np.sum((x - Xhat) ** 2) + l2 * (np.sum(P ** 2) + np.sum(Q ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm just going to do a little sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.121366454959534e-28\n",
      "0.0\n",
      "62301.46837253707\n",
      "62301.46837253707\n",
      "568809.3737568273\n",
      "568809.3737568273\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "P = np.random.randn(100, 3)\n",
    "Q = np.random.randn(200, 3)\n",
    "PQ = P @ Q.T\n",
    "X = {(i, j): PQ[i, j] for i, j in product (range(100), range(200))}\n",
    "print(loss(X, P, Q, 0))\n",
    "print(np.sum((PQ - P@Q.T) ** 2))\n",
    "P = 2 * P\n",
    "print(loss(X, P, Q, 0))\n",
    "print(np.sum((PQ - P@Q.T) ** 2))\n",
    "P = P + 1\n",
    "Q = Q + 1\n",
    "print(loss(X, P, Q, 0))\n",
    "print(np.sum((PQ - P@Q.T) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function looks reasonable. Great!\n",
    "\n",
    "Next, we implement the (analytic) gradient, as well as a slow numerical gradient to check our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getj(X, i):\n",
    "    js = [j for (k, j) in X.keys() if k == i]\n",
    "    return np.array(js)\n",
    "\n",
    "def geti(X, j):\n",
    "    iis = [i for (i, k) in X.keys() if k == j]\n",
    "    return np.array(iis)\n",
    "\n",
    "def grad(X, P, Q, l2):\n",
    "    dP = 2 * l2 * P\n",
    "    for i in range(P.shape[0]):\n",
    "        js = getj(X, i)\n",
    "        for j in js:\n",
    "            dP[i] += -2 * Q[j] * (X[(i, j)] - P[i].dot(Q[j]))\n",
    "    dQ = 2 * l2 * Q\n",
    "    for j, row in enumerate(Q):\n",
    "        iis = geti(X, j)\n",
    "        for i in iis:\n",
    "            dQ[j] += -2 * P[i] * (X[(i, j)] - P[i].dot(Q[j]))\n",
    "    return dP, dQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_grad(X, P, Q, l2, eps=1e-5):\n",
    "    dP = np.zeros(P.shape)\n",
    "    dQ = np.zeros(Q.shape)\n",
    "    for i in range(P.shape[0]):\n",
    "        for j in range(P.shape[1]):\n",
    "            P[i, j] += eps\n",
    "            pos = loss(X, P, Q, l2)\n",
    "            P[i, j] -= 2 * eps\n",
    "            neg = loss(X, P, Q, l2)\n",
    "            P[i, j] += eps\n",
    "            dP[i, j] = (pos - neg) / (2 * eps)\n",
    "    for i in range(Q.shape[0]):\n",
    "        for j in range(Q.shape[1]):\n",
    "            Q[i, j] += eps\n",
    "            pos = loss(X, P, Q, l2)\n",
    "            Q[i, j] -= 2 * eps\n",
    "            neg = loss(X, P, Q, l2)\n",
    "            Q[i, j] += eps\n",
    "            dQ[i, j] = (pos - neg) / (2 * eps)\n",
    "    return dP, dQ\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we perform a little random gradient check. I just ran the cell a few times and make sure the two numbers it outputs are always nearly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6341080462781927e-19\n",
      "1.9024847488473296e-19\n"
     ]
    }
   ],
   "source": [
    "P = np.random.randn(4, 2)\n",
    "Q = np.random.randn(8, 2)\n",
    "PQ = P @ Q.T\n",
    "X = {}\n",
    "for i in range(PQ.shape[0]):\n",
    "    for j in range(PQ.shape[1]):\n",
    "        if (i + j) % 2 == 0:\n",
    "            X[(i, j)] = PQ[i, j]\n",
    "l2 = 0.2\n",
    "P = np.random.randn(4, 2)\n",
    "Q = np.random.randn(8, 2)\n",
    "dPn, dQn = num_grad(X, P, Q, l2)\n",
    "dPa, dQa = grad(X, P, Q, l2)\n",
    "print(np.sum((dPa - dPn)** 2))\n",
    "print(np.sum((dQa - dQn) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've passed the gradient check! \n",
    "\n",
    "Now we just need a little bit more code to implement minibatch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of X, for minibatch sgd\n",
    "from random import sample\n",
    "def make_batch(X, size):\n",
    "    return {key: X[key] for key in sample(list(X), size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, k, l2, batch_size, step_size, epochs, P=None, Q=None, Xval=None, dims=None):\n",
    "    \"\"\"Factorize the matrix X using a rank k approximation.\n",
    "    \n",
    "    X - the matrix to factor, in sparse dictionary form\n",
    "    k - the rank of approximation\n",
    "    l2 - controls the l2 regularization amount\n",
    "    batch_size - the batch size for minibatch sgd\n",
    "    step_size - the step size for minibatch sgd\n",
    "    epochs - the number of epochs to run for,\n",
    "             where an epoch is defined to be\n",
    "             len(X) / batch_size many batches\n",
    "    P, Q - Initializations. If None, initialize randomly\n",
    "    Xval - held out data to compare performance on.\n",
    "    dims - (m, n) the dimensions of the implied matrix X\n",
    "           if none, the functions tries to infer\n",
    "    \"\"\"\n",
    "    if dims is None:\n",
    "        pshape = max(i for i, j in X.keys()) + 1\n",
    "        qshape = max(j for i, j in X.keys()) + 1\n",
    "    else:\n",
    "        pshape, qshape = dims\n",
    "    if P is None:\n",
    "        P = np.random.randn(pshape, k)\n",
    "    if Q is None:\n",
    "        Q = np.random.randn(qshape, k)\n",
    "    epoch_length = int(len(X) / batch_size) + 1\n",
    "    print(f\"Training for {epochs} epochs, each of {epoch_length} batches, each of size {batch_size}.\")\n",
    "    print(f\"Epoch -1: Training Loss = {loss(X, P, Q, l2)/len(X)}, Validation Loss = {loss(Xval, P, Q, 0)/len(Xval)}\")\n",
    "    for i in range(epochs):\n",
    "        for j in range(epoch_length):\n",
    "            print(f\"\\r========== Epoch {i} in progress...{j * 100 // epoch_length}% ==========\", end=\"\")\n",
    "            batch = make_batch(X, batch_size)\n",
    "            dP, dQ = grad(batch, P, Q, l2)\n",
    "            P -= step_size * dP\n",
    "            Q -= step_size * dQ\n",
    "        print(f\"\\r========== Epoch {i} in progress...100% ==========\")\n",
    "        if Xval is None:\n",
    "            print(f\"Epoch {i}: Loss = {loss(X, P, Q, l2)}\")\n",
    "        else:\n",
    "            # never use l2 penalty in validation loss\n",
    "            # normalize to be on same scale\n",
    "            print(f\"Epoch {i}: Training Loss = {loss(X, P, Q, l2)/len(X)}, Validation Loss = {loss(Xval, P, Q, 0)/len(Xval)}\")\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our code works on a tiny synthetic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.608090487723571e-28\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "P = np.random.randn(100, 3)\n",
    "Q = np.random.randn(200, 3)\n",
    "PQ = P @ Q.T\n",
    "X = {(i, j): PQ[i, j] for i, j in product(range(100), range(200))}\n",
    "cutoff = int(len(X) * .8)\n",
    "keys = list(X.keys())\n",
    "shuffle(keys)\n",
    "Xtrain = {key: X[key] for key in keys[:cutoff]}\n",
    "Xval = {key: X[key] for key in keys[cutoff:]}\n",
    "print(loss(X, P, Q, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs, each of 161 batches, each of size 100.\n",
      "Epoch -1: Training Loss = 5.894358217777715, Validation Loss = 6.094976374192005\n",
      "========== Epoch 0 in progress...100% ==========\n",
      "Epoch 0: Training Loss = 2.5171315979343523, Validation Loss = 2.781513141416761\n",
      "========== Epoch 1 in progress...100% ==========\n",
      "Epoch 1: Training Loss = 0.8978084382898048, Validation Loss = 1.0669522602969912\n",
      "========== Epoch 2 in progress...100% ==========\n",
      "Epoch 2: Training Loss = 0.10486428181981564, Validation Loss = 0.1297978833530837\n",
      "========== Epoch 3 in progress...100% ==========\n",
      "Epoch 3: Training Loss = 0.0063031305698191195, Validation Loss = 0.008025445259185008\n",
      "========== Epoch 4 in progress...100% ==========\n",
      "Epoch 4: Training Loss = 0.0014566148028996835, Validation Loss = 0.0013003985403829282\n",
      "========== Epoch 5 in progress...100% ==========\n",
      "Epoch 5: Training Loss = 0.0011699102320052077, Validation Loss = 0.0008398687124477883\n",
      "========== Epoch 6 in progress...100% ==========\n",
      "Epoch 6: Training Loss = 0.001153003664446624, Validation Loss = 0.0007840572116891126\n",
      "========== Epoch 7 in progress...100% ==========\n",
      "Epoch 7: Training Loss = 0.00122422825700083, Validation Loss = 0.0008898584222635501\n",
      "========== Epoch 8 in progress...100% ==========\n",
      "Epoch 8: Training Loss = 0.001166559093671888, Validation Loss = 0.0008115129094247533\n",
      "========== Epoch 9 in progress...100% ==========\n",
      "Epoch 9: Training Loss = 0.0011532252350042996, Validation Loss = 0.0008055415965096089\n"
     ]
    }
   ],
   "source": [
    "P, Q = fit(Xtrain, Xval=Xval, k=3, l2=0.01, batch_size=100, step_size=0.01, epochs=10, dims=(100, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, great. The model has quickly and sucessfully driven both the training and validation losses close to zero.\n",
    "Thus, the code is most likely working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything seems to be in order, we're finally ready to try out the code on a real data set.\n",
    "I'm going to use the small MovieLens data set (`ml-latest-small`), available freely from GroupLens [here](https://grouplens.org/datasets/movielens/).\n",
    "This small data set consists of 100,000 ratings applied to 9,000 movies by 600 users.\n",
    "\n",
    "First, let's load the data and take a peek at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100836, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process this data frame into the dictionary format required by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_index</th>\n",
       "      <th>movie_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp  user_index  movie_index\n",
       "0       1        1     4.0  964982703           0            0\n",
       "1       1        3     4.0  964981247           0            2\n",
       "2       1        6     4.0  964982224           0            5\n",
       "3       1       47     5.0  964983815           0           43\n",
       "4       1       50     5.0  964982931           0           46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step: process data into sparse matrix dictionary form\n",
    "# for that I need to re-index the userid and movieid to be consecutive non-negative integers\n",
    "user_ids = sorted(list(set(df.userId)))\n",
    "movie_ids = sorted(list(set(df.movieId)))\n",
    "user_index_map = {key: val for val, key in enumerate(user_ids)}\n",
    "movie_index_map = {key: val for val, key in enumerate(movie_ids)}\n",
    "df['user_index'] = df.userId.map(user_index_map)\n",
    "df['movie_index'] = df.movieId.map(movie_index_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary matrix\n",
    "keys = zip(df.user_index, df.movie_index)\n",
    "vals = df.rating\n",
    "X = {(i, j): x for (i, j), x in zip(keys, vals)}\n",
    "\n",
    "# split data\n",
    "cutoff = int(len(X) * .8)\n",
    "keys = list(X)\n",
    "shuffle(keys)\n",
    "Xtrain = {key: X[key] for key in keys[:cutoff]}\n",
    "Xval = {key: X[key] for key in keys[cutoff:]}\n",
    "dims = df.user_index.max() + 1, df.movie_index.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're ready to fit the model. To do so, we'd run the follwing code:\n",
    "```\n",
    "P, Q = fit(Xtrain, Xval=Xval, k=200, dims=dims, l2=0.01, batch_size=100, step_size=0.001, epochs=a_really_big_number)\n",
    "```\n",
    "I've already done this elsewhere, since it takes forever to run, and I want this notebook to execute quickly.\n",
    "\n",
    "So I'll just load up the pretrained $P$ and $Q$ matrices. We have a pretty low MSE on the validation data. Nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5958826178605978"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, Q = np.load('P.npy'), np.load('Q.npy')\n",
    "loss(Xval, P, Q, 0) / len(Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some individual predictions. Below, we can see that the predicted ratings are pretty good. They are generally within 1 star (the rating unit) of the true rating. Though sometimes they can be quite far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(451, 275, 5.0, 4.644598369878881),\n",
       " (312, 559, 1.0, 1.5419352060574192),\n",
       " (336, 23, 5.0, 3.334300660089183),\n",
       " (327, 7061, 4.0, 3.4669705536037108),\n",
       " (447, 3490, 3.0, 3.0905088672890457),\n",
       " (584, 1839, 4.0, 2.76536382219863),\n",
       " (364, 5363, 2.0, 1.9991478770895572),\n",
       " (533, 7823, 2.0, 2.244419421995681),\n",
       " (606, 1157, 4.0, 3.5923271202926546),\n",
       " (447, 8022, 3.0, 1.2447174489819972),\n",
       " (609, 7123, 3.0, 2.8549683445071605),\n",
       " (605, 2226, 3.0, 2.605996177558488),\n",
       " (602, 2312, 4.0, 3.823142108372504),\n",
       " (359, 1198, 2.0, 2.7603552309162906),\n",
       " (135, 540, 4.0, 2.890747829885643),\n",
       " (411, 398, 4.0, 4.399439156786133),\n",
       " (90, 1043, 3.5, 3.5071153854308044),\n",
       " (540, 254, 3.0, 3.56781811791424),\n",
       " (436, 442, 3.0, 2.924777167945934),\n",
       " (67, 1570, 3.0, 3.0071861743043464)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This makes a table of sorts of the real and predicted ratings for the val set\n",
    "[(user, movie, actual, P[user].dot(Q[movie])) for ((user, movie), actual) in Xval.items()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is cool.\n",
    "But how do we generate predictions for new users? Well, one way is to add the user to the data and retrain the whole model from scratch.\n",
    "Clearly, that is not going to be very efficient. Of course, we could try to reuse the existing $P$ and $Q$ to give a good initalization\n",
    "(a warm-start, as it is sometimes called) to the \n",
    "new training run. Taking this idea to an extreme, we could leave $P$ and $Q$ as completely fixed as possible, and only try to learn an approximate new\n",
    "latent vector for the new user. This new vector, call it $p$, is supposed to satisfy $x \\approx pQ$, where $x$ is the row of ratings for the new user,\n",
    "and $Q$ is the already-learned representation of the items. So, in other words, we want to minimize (wrt $p$ only)\n",
    "\n",
    "$$\n",
    "||Q^Tp^T - x^T||^2 + \\lambda ||p|| ^ 2\n",
    "$$\n",
    "\n",
    "But this is just ridge regression(!), where $Q^T = X$, $p^T = \\beta$, and $x^T = y$. So now we have an easy way to generate predictions for a hypothetical new user. For example, we could have a guest user input ratings for a few movies, and then we could output movie recommendations in real time, without having to wait for the model to retrain.\n",
    "\n",
    "If we wanted to make this idea really efficient, I'm sure we could cache a matrix decomposition (or similarly a matrix inverse, but something like the Cholesky factorization usually has better numerical properties) so that we only have to do a few matrix multiplications to get the recommendations. For our purposes, I'll just use scikit-learn's implementation of ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Persuasion (1995)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up the movie info, set the index to movieId (for fast lookup)\n",
    "# and then test out a look up (should give Persuasion)\n",
    "movie_df = pd.read_csv('ml-latest-small/movies.csv')\n",
    "movie_df = movie_df.set_index('movieId')\n",
    "movie_df.loc[28].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ids for all the children's movies\n",
    "children_ids = movie_df[movie_df.genres.str.contains(\"Children\")].index.values\n",
    "# Get the ids for all the dramas\n",
    "drama_ids = movie_df[movie_df.genres.str.contains(\"Drama\")].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Forrest Gump (1994)', 6.383852472927765),\n",
       " ('Terminator 2: Judgment Day (1991)', 6.2847574267970145),\n",
       " ('Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)',\n",
       "  6.275125559269302),\n",
       " ('Fugitive, The (1993)', 6.228253213211051),\n",
       " ('Shrek (2001)', 6.171320227058867),\n",
       " ('Shawshank Redemption, The (1994)', 6.17112826012575),\n",
       " ('Star Wars: Episode IV - A New Hope (1977)', 6.1668186290012805),\n",
       " ('Star Wars: Episode VI - Return of the Jedi (1983)', 6.14994676458946),\n",
       " ('Hoop Dreams (1994)', 6.1385696988648135),\n",
       " ('Postman, The (Postino, Il) (1994)', 6.04826334311373),\n",
       " ('Speed (1994)', 5.986584934056694),\n",
       " ('Princess Bride, The (1987)', 5.979179875450452),\n",
       " ('Lawrence of Arabia (1962)', 5.949116095984733),\n",
       " ('Braveheart (1995)', 5.94549892748534),\n",
       " ('Lion King, The (1994)', 5.942644653553394),\n",
       " ('Good Will Hunting (1997)', 5.92669912205136),\n",
       " ('Star Wars: Episode V - The Empire Strikes Back (1980)', 5.923137999597239),\n",
       " ('Terminator, The (1984)', 5.917200585414389),\n",
       " ('Streetcar Named Desire, A (1951)', 5.901246560675714),\n",
       " ('Back to the Future (1985)', 5.896419062656717)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Let's suppose our hypothetical new user has given 5 star ratings to the 10 children's movies\n",
    "ratings = {movie_index_map[key]: 5.0 for key in children_ids[:10]}\n",
    "\n",
    "# Get the rows of Q for the corresponding movies\n",
    "Q_filter = Q[list(ratings.keys())]\n",
    "x = np.array(list(ratings.values()))\n",
    "\n",
    "# Fit the ridge model and get the ratings predictions out\n",
    "full_ratings = Ridge(alpha=1, fit_intercept=False).fit(Q_filter, x).predict(Q)\n",
    "# associate the title to each rating\n",
    "full_ratings = [(movie_df.loc[movie_ids[i]].title, rating) for i, rating in enumerate(full_ratings)]\n",
    "# Sort by highest rating to lowest\n",
    "full_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 20 recommendations\n",
    "full_ratings[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Forrest Gump (1994)', 6.484102530807535),\n",
       " ('Shawshank Redemption, The (1994)', 6.296902511472191),\n",
       " (\"Schindler's List (1993)\", 6.117397319506058),\n",
       " ('Usual Suspects, The (1995)', 6.072240703293817),\n",
       " ('Postman, The (Postino, Il) (1994)', 6.020337246777842),\n",
       " ('Braveheart (1995)', 5.953058316887174),\n",
       " ('Silence of the Lambs, The (1991)', 5.868094031620335),\n",
       " ('Hoop Dreams (1994)', 5.781996588356677),\n",
       " ('Shrek (2001)', 5.773151324852257),\n",
       " ('American History X (1998)', 5.745147853064867),\n",
       " ('Lion King, The (1994)', 5.718513660705401),\n",
       " ('Star Wars: Episode VI - Return of the Jedi (1983)', 5.71546782704302),\n",
       " ('Matrix, The (1999)', 5.697166598602758),\n",
       " ('Dances with Wolves (1990)', 5.696776595603996),\n",
       " ('Fugitive, The (1993)', 5.681671806559443),\n",
       " ('Terminator 2: Judgment Day (1991)', 5.67062190066647),\n",
       " ('Green Mile, The (1999)', 5.6598480424153985),\n",
       " ('Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)',\n",
       "  5.656563000976246),\n",
       " ('Saving Private Ryan (1998)', 5.635958102309601),\n",
       " ('Apollo 13 (1995)', 5.635348421193351)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do it again with someone who likes dramas\n",
    "\n",
    "# Now the hypothetical new user has given 5 star ratings to the 10 dramas\n",
    "ratings = {movie_index_map[key]: 5.0 for key in drama_ids[:10]}\n",
    "\n",
    "# Get the rows of Q for the corresponding movies\n",
    "Q_filter = Q[list(ratings.keys())]\n",
    "x = np.array(list(ratings.values()))\n",
    "\n",
    "# Fit the ridge model and get the ratings predictions out\n",
    "full_ratings = Ridge(alpha=1, fit_intercept=False).fit(Q_filter, x).predict(Q)\n",
    "# associate the title to each rating\n",
    "full_ratings = [(movie_df.loc[movie_ids[i]].title, rating) for i, rating in enumerate(full_ratings)]\n",
    "# Sort by highest rating to lowest\n",
    "full_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 20 recommendations\n",
    "full_ratings[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that the recommendations are pretty weird.\n",
    "Even though we had our hypothetical user rated a bunch of children's movies highly, our recommender system\n",
    "seems to just output an arbitrary assortment of highly acclaimed films.\n",
    "And a similar thing happens for the person who loves dramas.\n",
    "\n",
    "The problem is that some movies are universally acclaimed, according to the data.\n",
    "So the model learns give these movies long latent vectors, which means that everyone ends up rating these movies highly, regardless of their own\n",
    "particular preferences. \n",
    "\n",
    "There are modifications we can make to the algorithm to help account for this problem.\n",
    "For example, we can add an additional parameter for each movie that accounts for the average rating of the movie.\n",
    "Then the dot product of the user and movie vectors accounts for a deviation from the usual rating.\n",
    "We can extend the same idea to the users. Since some users are more likely to give higher ratings overall, and some are more likely\n",
    "to give lower ratings, accounting for the average behavior of the user can also help isolate the interaction between user and movie.\n",
    "Once we've separated out the interaction from the average behavior of the users and movies, then we can adjust our recommendations\n",
    "accordingly.\n",
    "\n",
    "I won't be implementing these improvements in my code right now, since I think this tutorial is long enough.\n",
    "If you want to see how these modified algorithms perform, or if you want to create a recommender system to use in practice,\n",
    "I'd recommend the [Surprise scikit.](http://surpriselib.com)\n",
    "\n",
    "Well, our recommendations didn't turn out to be amazing, but hopefully if you read this all then you've learned that there's really nothing\n",
    "too complicated about the core ideas behind matrix factorization recommender systems. Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
